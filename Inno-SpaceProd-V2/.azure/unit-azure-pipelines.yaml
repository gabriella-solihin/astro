# Python package
# Create and test a Python package on multiple Python versions.
# Add steps that analyze code, save the dist with the build record, publish to a PyPI-compatible index, and more:
# https://docs.microsoft.com/azure/devops/pipelines/languages/python

trigger:
- master
- develop
- develop_parallelization


jobs:
  - job: "space_prod_integ_test_part_1_clustering"
    timeoutInMinutes: 120


    pool: Azure Pipelines
    strategy:
      matrix:
        Python37:
          python.version: '3.7'


    steps:
    - task: UsePythonVersion@0
      inputs:
        versionSpec: '$(python.version)'
      displayName: 'Use Python $(python.version)'

    - checkout: self
      persistCredentials: true
      clean: true

    - script: |
        pip install -U pip==20.2.3
        pip install pytest pytest-cov requests setuptools wheel pytest-progress==1.2.4
        pip install -e .
      displayName: 'Load Python Dependencies'


    - script: |
        
        # set required non-sensitive variables
        db_host="https://adb-5563199756815371.11.azuredatabricks.net/"
        db_org_id="5563199756815371"
        db_cluster_id="0326-202913-d2wge96m" 
        db_port="15001"
        
        # get sensivite variables
        db_token=$(DATABRICKS-TOKEN)
        
        # setup initial configuration 
        
        echo "y
        $db_host
        $db_token" | databricks configure --token
         
        # add required configuration parameters to the databricks-connect config
        
        echo "
        {
          \"host\": \"$db_host\",
          \"token\": \"$db_token\",
          \"cluster_id\": \"$db_cluster_id\",
          \"org_id\": \"$db_org_id\",
          \"port\": \"$db_port\"
        }
        " > ~/.databricks-connect        
        
        # spit the config into logs 
        
        echo "$(cat ~/.databricks-connect)"        

      displayName: 'Configure DB CLI'

    - script: |
        python3 setup.py bdist_wheel
        ls dist/
      displayName: 'Build Python Wheel for Libs'

    - script: |
        # tests/src/elasticity/test_elasticity_model_run.py
        export SPACE_USE_TEST_SPARK_CONF=1
        # export SPACE_ADD_WHEEL_TO_SPARK=1
        python -m pytest -s -r A -vv --tb=native --junitxml=junit/test-results.xml --cov=spaceprod/src --cov-report=xml --cov-report=html --cov-report term -k 'not integration_space_prod' --durations=0 --cov-config=.coveragerc_unit --show-progress tests
      displayName: 'Unit Test'
      timeoutInMinutes: 120

    - task: PublishCodeCoverageResults@1
      inputs:
        codeCoverageTool: Cobertura
        summaryFileLocation: '$(System.DefaultWorkingDirectory)/**/coverage.xml'
        reportDirectory: '$(System.DefaultWorkingDirectory)/**/htmlcov'

    - task: PublishTestResults@2
      condition: succeededOrFailed()
      inputs:
        testResultsFiles: '**/test-*.xml'
        failTaskOnFailedTests: true
        publishRunAttachments: true

    - script: |
        mkdir -p $(Build.BinariesDirectory)/libraries/python/libs
        cp dist/*.* $(Build.BinariesDirectory)/libraries/python/libs

      displayName: 'Get Changes'

    - task: ArchiveFiles@2
      inputs:
        rootFolderOrFile: '$(Build.BinariesDirectory)'
        includeRootFolder: false
        archiveType: 'zip'
        archiveFile: '$(Build.ArtifactStagingDirectory)/spaceprod.zip'
        replaceExistingArchive: true

    - task: PublishBuildArtifacts@1
      inputs:
        ArtifactName: 'DatabricksBuild'

    - script: |
        # fail coverage if under 80%
        coverage xml  --fail-under=80
      displayName: 'Test coverage must be 80% or more!'